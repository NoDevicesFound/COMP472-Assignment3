{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # feature frequency\n",
    "    tokenized_text = (vectorizer.fit_transform(text)).toarray()\n",
    "    # feature name\n",
    "    tokenized_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    row, column = tokenized_text.shape\n",
    "    features_text = np.zeros((row, column), dtype=object)\n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            # tuple containing feature name and frequency\n",
    "            features_text[r][c] = (tokenized_names[c], tokenized_text[r][c])\n",
    "            \n",
    "    return features_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(q1_label):\n",
    "    number_of_yes, number_of_no = 0, 0\n",
    "    \n",
    "    # per class prior probabilities\n",
    "    for i in range(len(q1_label)):\n",
    "        # probability that the class is \"yes\"\n",
    "        if (q1_label[i] == \"yes\"):\n",
    "            number_of_yes = number_of_yes + 1\n",
    "        # probability that the class is \"no\"\n",
    "        else:\n",
    "            number_of_no = number_of_no + 1\n",
    "    \n",
    "    probability_of_yes, probability_of_no = (number_of_yes/len(q1_label)), (number_of_no/len(q1_label)) \n",
    "    return probability_of_yes, probability_of_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probabilities(text, q1_label, SMOOTHING):\n",
    "    row = len(set(q1_label))\n",
    "    column = text.shape[1]\n",
    "    frequencies = np.zeros((row, column), dtype=int)\n",
    "    probabilities = np.zeros((row, column), dtype=object)\n",
    "    smoothing_vocabulary = SMOOTHING * column\n",
    "    \n",
    "    total_row, total_column = text.shape\n",
    "    total_yes, total_no = 0, 0\n",
    "    for r in range(total_row): \n",
    "        for c in range(total_column):\n",
    "            # \"yes\" class\n",
    "            if (q1_label[r] == \"yes\"):\n",
    "                total_yes = total_yes + text[r][c][1]\n",
    "                frequencies[0][c] = frequencies[0][c] + text[r][c][1]\n",
    "            # \"no\" class\n",
    "            else:\n",
    "                total_no = total_no + text[r][c][1]\n",
    "                frequencies[1][c] = frequencies[1][c] + text[r][c][1]\n",
    "                \n",
    "    for r in range(total_row): \n",
    "        for c in range(total_column):\n",
    "            # \"yes\" class\n",
    "            probability = (frequencies[0][c] + SMOOTHING)/(total_yes + smoothing_vocabulary)\n",
    "            probabilities[0][c] = (text[r][c][0], probability)\n",
    "            # \"no\" class\n",
    "            probability = (frequencies[1][c] + SMOOTHING)/(total_no + smoothing_vocabulary)\n",
    "            probabilities[1][c] = (text[r][c][0], probability)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_vocabulary(conditionals, text):\n",
    "    matches = []\n",
    "    \n",
    "    text_column = text.shape[1]\n",
    "    conditional_column = conditionals.shape[1]\n",
    "    \n",
    "    for tc in range(text_column):\n",
    "        for cc in range(conditional_column):\n",
    "            if (text[0][tc][0] == conditionals[0][cc][0]):\n",
    "                matches.append((tc, cc))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prediction(yes, no, conditionals, text, matches): \n",
    "    text_row = text.shape[0]\n",
    "    prediction = []\n",
    "    scores = []\n",
    "    \n",
    "    for tr in range(text_row):\n",
    "        score_yes, score_no = math.log10(yes), math.log10(no)\n",
    "        for i in range(len(matches)):\n",
    "            text_index = matches[i][0]\n",
    "            conditional_index = matches[i][1]\n",
    "            if (text[tr][text_index][1] > 0):\n",
    "                score_yes = score_yes + math.log10(conditionals[0][conditional_index][1])\n",
    "                score_no = score_no + math.log10(conditionals[1][conditional_index][1])\n",
    "        if (score_yes > score_no):\n",
    "            prediction.append(\"yes\")\n",
    "            scores.append(score_yes)\n",
    "        else:\n",
    "            prediction.append(\"no\")\n",
    "            scores.append(score_no)\n",
    "    \n",
    "    return prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    precision_no = precision_score(y_true, y_pred, pos_label=\"no\")\n",
    "    precision_yes = precision_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    recall_no = recall_score(y_true, y_pred, pos_label=\"no\")\n",
    "    recall_yes = recall_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    f1_no = f1_score(y_true, y_pred, pos_label=\"no\")\n",
    "    f1_yes = f1_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(training_file, testing_file, SMOOTHING):\n",
    "    # training: hypothesis and evidence\n",
    "    train_dataset = (pd.read_csv(training_file, sep='\\t')).to_numpy()\n",
    "    train_tweet_id = train_dataset[:,0]\n",
    "    train_text = word_tokenization(train_dataset[:,1])\n",
    "    train_q1_label = train_dataset[:,2]\n",
    "    \n",
    "    # prior probabilities\n",
    "    yes, no = prior_probabilities(train_q1_label)\n",
    "    \n",
    "    # conditional probabilities \n",
    "    conditionals = conditional_probabilities(train_text, train_q1_label, SMOOTHING)\n",
    "    \n",
    "    # testing: hypothesis and evidence\n",
    "    test_dataset = (pd.read_csv(testing_file, sep='\\t')).to_numpy()\n",
    "    test_tweet_id = test_dataset[:,0]\n",
    "    test_text = word_tokenization(test_dataset[:,1])\n",
    "    test_q1_label = test_dataset[:,2]\n",
    "    \n",
    "    # return index of matching words\n",
    "    matches = trim_vocabulary(conditionals, test_text)\n",
    "    \n",
    "    # prediction\n",
    "    prediction, scores = class_prediction(yes, no, conditionals, test_text, matches)\n",
    "    \n",
    "    metrics(test_q1_label, prediction)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.79      0.52      0.63        21\n",
      "         yes       0.75      0.91      0.82        33\n",
      "\n",
      "    accuracy                           0.76        54\n",
      "   macro avg       0.77      0.72      0.73        54\n",
      "weighted avg       0.76      0.76      0.75        54\n",
      "\n",
      "[[11 10]\n",
      " [ 3 30]]\n",
      "[-37.400338409905764, -99.28635003235577, -76.38734877726633, -45.43719447336773, -57.416285508128894, -49.873840524044475, -73.69293513540636, -92.35687763531257, -59.142314346480504, -54.093090907366374, -11.99878441035089, -84.08625890512192, -41.764869386521156, -40.475380053308925, -105.61550298987595, -30.195596193149193, -62.66466630697913, -52.410561038630036, -40.04253422835775, -68.93258557587288, -20.383512260158234, -69.46420402174364, -23.80234808200825, -56.03601625756487, -95.07177060711327, -74.10462888322068, -80.08121934657244, -63.177351273480056, -45.14315221666656, -24.95729817249658, -30.340010155623872, -37.21571960674294, -10.257203267129931, -41.764869386521156, -13.296345924932384, -98.49671835064116, -26.385929269882258, -16.50081426534687, -52.59184777658234, -49.76395957130869, -71.05793182473283, -87.53953799272031, -85.80008320600452, -23.72583969995927, -81.30218342428094, -111.24128434935523, -40.58654719587675, -26.86642620738602, -85.27885219810607, -84.5676356551837, -41.12446742911083, -88.35665278762433, -47.11856437723094, -81.3225001246312]\n"
     ]
    }
   ],
   "source": [
    "# processing datasets\n",
    "training = '../data/covid_training.tsv'\n",
    "testing = '../data/covid_test_public.tsv'\n",
    "data_processing(training, testing, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
