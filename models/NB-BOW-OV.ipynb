{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # feature frequency\n",
    "    tokenized_text = (vectorizer.fit_transform(text)).toarray()\n",
    "    # feature name\n",
    "    tokenized_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    row, column = tokenized_text.shape\n",
    "    features_text = np.zeros((row, column), dtype=object)\n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            # tuple containing feature name and frequency\n",
    "            features_text[r][c] = (tokenized_names[c], tokenized_text[r][c])\n",
    "    return features_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(q1_label):\n",
    "    number_of_yes, number_of_no = 0, 0\n",
    "    \n",
    "    # per class prior probabilities\n",
    "    for i in range(len(q1_label)):\n",
    "        # probability that the class is \"yes\"\n",
    "        if (q1_label[i] == \"yes\"):\n",
    "            number_of_yes = number_of_yes + 1\n",
    "        # probability that the class is \"no\"\n",
    "        else:\n",
    "            number_of_no = number_of_no + 1\n",
    "    \n",
    "    probability_of_yes, probability_of_no = (number_of_yes/len(q1_label)), (number_of_no/len(q1_label)) \n",
    "    return probability_of_yes, probability_of_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probabilities(text, q1_label, SMOOTHING):\n",
    "    row = len(set(q1_label))\n",
    "    column = text.shape[1]\n",
    "    probabilities = np.zeros((row, column), dtype=object)\n",
    "    smoothing_vocabulary = SMOOTHING * column\n",
    "    \n",
    "    # total number of words in each class\n",
    "    total_row, total_column = text.shape\n",
    "    total_words_in_yes, total_words_in_no = 0, 0\n",
    "    for r in range(total_row):\n",
    "        # total number of words in yes\n",
    "        if (q1_label[r] == \"yes\"):\n",
    "            for c in range(total_column):\n",
    "                total_words_in_yes = total_words_in_yes + text[r][c][1]\n",
    "        # total number of words in no\n",
    "        else: \n",
    "            for c in range(total_column):\n",
    "                total_words_in_no = total_words_in_no + text[r][c][1]\n",
    "    \n",
    "    # frequency of word per class\n",
    "    for c in range(column):\n",
    "        number_of_word_in_yes, number_of_word_in_no = 0, 0\n",
    "        for r in range(row):\n",
    "            # frequency of word in yes\n",
    "            if (q1_label[r] == \"yes\"):\n",
    "                number_of_word_in_yes = number_of_word_in_yes + text[r][c][1]\n",
    "            # frequency of word in no\n",
    "            else:\n",
    "                number_of_word_in_no = number_of_word_in_no + text[r][c][1]\n",
    "        \n",
    "        #row 1 = yes\n",
    "        probability_with_class_yes = (number_of_word_in_yes + SMOOTHING)/(total_words_in_yes + smoothing_vocabulary)\n",
    "        probabilities[0][c] = (text[0][c][0], probability_with_class_yes)\n",
    "        #row 2 = no\n",
    "        probability_with_class_no = (number_of_word_in_no + SMOOTHING)/(total_words_in_no + smoothing_vocabulary)\n",
    "        probabilities[1][c] = (text[0][c][0], probability_with_class_no)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_vocabulary(conditionals, text):\n",
    "    matches = []\n",
    "    \n",
    "    text_column = text.shape[1]\n",
    "    conditional_column = conditionals.shape[1]\n",
    "    \n",
    "    for tc in range(text_column):\n",
    "        for cc in range(conditional_column):\n",
    "            if (text[0][tc][0] == conditionals[0][cc][0]):\n",
    "                matches.append((tc, cc))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prediction(yes, no, conditionals, text, matches): \n",
    "    text_row = text.shape[0]\n",
    "    prediction = []\n",
    "    \n",
    "    for tr in range(text_row):\n",
    "        score_yes, score_no = math.log(yes), math.log(no)\n",
    "        for i in range(len(matches)):\n",
    "            text_index = matches[i][0]\n",
    "            conditional_index = matches[i][1]\n",
    "            if (text[tr][text_index][1] > 0):\n",
    "                score_yes = score_yes + math.log(conditionals[0][conditional_index][1])\n",
    "                score_no = score_no + math.log(conditionals[1][conditional_index][1])\n",
    "        if (score_yes > score_no):\n",
    "            prediction.append(\"yes\")\n",
    "        else:\n",
    "            prediction.append(\"no\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(training_file, testing_file, SMOOTHING):\n",
    "    # training: hypothesis and evidence\n",
    "    train_dataset = (pd.read_csv(training_file, sep='\\t')).to_numpy()\n",
    "    train_tweet_id = train_dataset[:,0]\n",
    "    train_text = word_tokenization(train_dataset[:,1])\n",
    "    train_q1_label = train_dataset[:,2]\n",
    "    \n",
    "    # prior probabilities\n",
    "    yes, no = prior_probabilities(train_q1_label)\n",
    "    \n",
    "    # conditional probabilities \n",
    "    conditionals = conditional_probabilities(train_text, train_q1_label, SMOOTHING)\n",
    "    \n",
    "    # testing: hypothesis and evidence\n",
    "    test_dataset = (pd.read_csv(testing_file, sep='\\t')).to_numpy()\n",
    "    test_tweet_id = test_dataset[:,0]\n",
    "    test_text = word_tokenization(test_dataset[:,1])\n",
    "    test_q1_label = test_dataset[:,2]\n",
    "    \n",
    "    \n",
    "    # return index of matching words\n",
    "    matches = trim_vocabulary(conditionals, test_text)\n",
    "    \n",
    "    # prediction\n",
    "    prediction = class_prediction(yes, no, conditionals, test_text, matches)\n",
    "    \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "# processing datasets\n",
    "training = '../data/covid_training.tsv'\n",
    "testing = '../data/covid_test_public.tsv'\n",
    "data_processing(training, testing, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
