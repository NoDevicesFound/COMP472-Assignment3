{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(text):\n",
    "    vectorizer = CountVectorizer(min_df=2)\n",
    "    \n",
    "    # feature frequency\n",
    "    tokenized_text = (vectorizer.fit_transform(text)).toarray()\n",
    "    # feature name\n",
    "    tokenized_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    row, column = tokenized_text.shape\n",
    "    features_text = np.zeros((row, column), dtype=object)\n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            # tuple containing feature name and frequency\n",
    "            features_text[r][c] = (tokenized_names[c], tokenized_text[r][c])\n",
    "            \n",
    "    return features_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(q1_label):\n",
    "    number_of_yes, number_of_no = 0, 0\n",
    "    \n",
    "    # per class prior probabilities\n",
    "    for i in range(len(q1_label)):\n",
    "        # probability that the class is \"yes\"\n",
    "        if (q1_label[i] == \"yes\"):\n",
    "            number_of_yes = number_of_yes + 1\n",
    "        # probability that the class is \"no\"\n",
    "        else:\n",
    "            number_of_no = number_of_no + 1\n",
    "    \n",
    "    probability_of_yes, probability_of_no = (number_of_yes/len(q1_label)), (number_of_no/len(q1_label)) \n",
    "    return probability_of_yes, probability_of_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probabilities(text, q1_label, SMOOTHING):\n",
    "    row = len(set(q1_label))\n",
    "    column = text.shape[1]\n",
    "    frequencies = np.zeros((row, column), dtype=int)\n",
    "    probabilities = np.zeros((row, column), dtype=object)\n",
    "    smoothing_vocabulary = SMOOTHING * column\n",
    "    \n",
    "    total_row, total_column = text.shape\n",
    "    total_yes, total_no = 0, 0\n",
    "    for r in range(total_row): \n",
    "        for c in range(total_column):\n",
    "            # \"yes\" class\n",
    "            if (q1_label[r] == \"yes\"):\n",
    "                total_yes = total_yes + text[r][c][1]\n",
    "                frequencies[0][c] = frequencies[0][c] + text[r][c][1]\n",
    "            # \"no\" class\n",
    "            else:\n",
    "                total_no = total_no + text[r][c][1]\n",
    "                frequencies[1][c] = frequencies[1][c] + text[r][c][1]\n",
    "                \n",
    "    for r in range(total_row): \n",
    "        for c in range(total_column):\n",
    "            # \"yes\" class\n",
    "            probability = (frequencies[0][c] + SMOOTHING)/(total_yes + smoothing_vocabulary)\n",
    "            probabilities[0][c] = (text[r][c][0], probability)\n",
    "            # \"no\" class\n",
    "            probability = (frequencies[1][c] + SMOOTHING)/(total_no + smoothing_vocabulary)\n",
    "            probabilities[1][c] = (text[r][c][0], probability)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_vocabulary(conditionals, text):\n",
    "    matches = []\n",
    "    \n",
    "    text_column = text.shape[1]\n",
    "    conditional_column = conditionals.shape[1]\n",
    "    \n",
    "    for tc in range(text_column):\n",
    "        for cc in range(conditional_column):\n",
    "            if (text[0][tc][0] == conditionals[0][cc][0]):\n",
    "                matches.append((tc, cc))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_prediction(yes, no, conditionals, text, matches): \n",
    "    text_row = text.shape[0]\n",
    "    prediction = []\n",
    "    scores = []\n",
    "    \n",
    "    for tr in range(text_row):\n",
    "        score_yes, score_no = math.log10(yes), math.log10(no)\n",
    "        for i in range(len(matches)):\n",
    "            text_index = matches[i][0]\n",
    "            conditional_index = matches[i][1]\n",
    "            if (text[tr][text_index][1] > 0):\n",
    "                score_yes = score_yes + math.log10(conditionals[0][conditional_index][1])\n",
    "                score_no = score_no + math.log10(conditionals[1][conditional_index][1])\n",
    "        if (score_yes > score_no):\n",
    "            prediction.append(\"yes\")\n",
    "            scores.append(score_yes)\n",
    "        else:\n",
    "            prediction.append(\"no\")\n",
    "            scores.append(score_no)\n",
    "    \n",
    "    return prediction, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred, y_pred_score, test_dataset):\n",
    "    precision_no = precision_score(y_true, y_pred, pos_label=\"no\")\n",
    "    precision_yes = precision_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    recall_no = recall_score(y_true, y_pred, pos_label=\"no\")\n",
    "    recall_yes = recall_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    f1_no = f1_score(y_true, y_pred, pos_label=\"no\")\n",
    "    f1_yes = f1_score(y_true, y_pred, pos_label=\"yes\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    output_trace_file = open(\"../output/trace_NB-BOW-FV.txt\", \"w\")\n",
    "    for id, predict, score, result in zip(test_dataset[:,0], y_pred, y_pred_score, test_dataset[:,2]):\n",
    "        output_trace_file.writelines(('{}  {}  {}  {}  {}\\r').format(id, predict, score, result,  (\"correct\" if (predict == result) else \"wrong\")))\n",
    "    output_trace_file.close()\n",
    "    \n",
    "    output_eval_file = open(\"../output/eval_NB-BOW-FV.txt\", \"w\")\n",
    "    output_eval_file.write(\"Accuracy_score: \" + accuracy.astype(str))\n",
    "    output_eval_file.write(\"\\nprecision_score yes: \" + precision_yes.astype(str) + \"\\tprecision_score no: \" + precision_no.astype(str))\n",
    "    output_eval_file.write(\"\\nrecall_score yes: \" + recall_yes.astype(str) + \"\\trecall_score no: \" + recall_no.astype(str))\n",
    "    output_eval_file.write(\"\\nf1_score yes: \" + f1_yes.astype(str) + \"\\tf1_score no: \" + f1_no.astype(str))\n",
    "    output_eval_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(training_file, testing_file, SMOOTHING):\n",
    "    # training: hypothesis and evidence\n",
    "    train_dataset = (pd.read_csv(training_file, sep='\\t')).to_numpy()\n",
    "    train_tweet_id = train_dataset[:,0]\n",
    "    train_text = word_tokenization(train_dataset[:,1])\n",
    "    train_q1_label = train_dataset[:,2]\n",
    "    \n",
    "    # prior probabilities\n",
    "    yes, no = prior_probabilities(train_q1_label)\n",
    "    \n",
    "    # conditional probabilities \n",
    "    conditionals = conditional_probabilities(train_text, train_q1_label, SMOOTHING)\n",
    "    \n",
    "    # testing: hypothesis and evidence\n",
    "    test_dataset = (pd.read_csv(testing_file, sep='\\t')).to_numpy()\n",
    "    test_tweet_id = test_dataset[:,0]\n",
    "    test_text = word_tokenization(test_dataset[:,1])\n",
    "    test_q1_label = test_dataset[:,2]\n",
    "    \n",
    "    # return index of matching words\n",
    "    matches = trim_vocabulary(conditionals, test_text)\n",
    "    \n",
    "    # prediction\n",
    "    prediction, scores = class_prediction(yes, no, conditionals, test_text, matches)\n",
    "    \n",
    "    metrics(test_q1_label, prediction, scores, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.80      0.57      0.67        21\n",
      "         yes       0.77      0.91      0.83        33\n",
      "\n",
      "    accuracy                           0.78        54\n",
      "   macro avg       0.78      0.74      0.75        54\n",
      "weighted avg       0.78      0.78      0.77        54\n",
      "\n",
      "[[12  9]\n",
      " [ 3 30]]\n"
     ]
    }
   ],
   "source": [
    "# processing datasets\n",
    "training = '../data/covid_training.tsv'\n",
    "testing = '../data/covid_test_public.tsv'\n",
    "data_processing(training, testing, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
